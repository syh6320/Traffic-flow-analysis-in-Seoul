opt_cp <- cptable[with(cptable,which.min(xerror)),"CP"]
# prune tree
opt_tree <- prune(tree,cp = opt_cp)
# let's apply it on test data
est_demo <- predict(object = opt_tree, newdata = test_data)
# let's check the performance of opt_tre
accuracy.meas(response = test_data$demo, predicted = est_demo[,2], threshold = 0.5)
# let's look at roc, auc
roc.curve(response = test_data$demo, predicted = est_demo[,2], plotit = T)
opt_cp <- cptable[with(cptable,min(which((xerror - xstd) < min(xerror)))),"CP"]
# let's find a appropriate cp
cptable <- as.data.frame(tree$cptable)
opt_cp <- cptable[with(cptable,min(which((xerror - xstd) < min(xerror)))),"CP"]
# prune tree
opt_tree <- prune(tree,cp = opt_cp)
# let's apply it on test data
est_demo <- predict(object = opt_tree, newdata = test_data)
# let's check the performance of opt_tre
accuracy.meas(response = test_data$demo, predicted = est_demo[,2], threshold = 0.5)
# let's look at roc, auc
roc.curve(response = test_data$demo, predicted = est_demo[,2], plotit = T)
# let's find a appropriate cp
cptable <- as.data.frame(tree$cptable)
# opt_cp <- cptable[with(cptable,min(which((xerror - xstd) < min(xerror)))),"CP"]
opt_cp <- cptable[with(cptable, which.min(xerror)),"CP"]
# prune tree
opt_tree <- prune(tree,cp = opt_cp)
# let's apply it on test data
est_demo <- predict(object = opt_tree, newdata = test_data)
# let's check the performance of opt_tre
accuracy.meas(response = test_data$demo, predicted = est_demo[,2], threshold = 0.5)
# let's look at roc, auc
roc.curve(response = test_data$demo, predicted = est_demo[,2], plotit = T)
# import grid data
grid_data <- read.csv(file = "data/grid_all.csv", stringsAsFactors = F)[,-1]
# let's just use rrate and speed to predict demo
grid_data <- subset(x = grid_data, select = -c(date,hour,grid))
# let's convert , hour, grid, demo to factors
# grid_data[,"hour"] <- as.factor(grid_data[,"hour"])
# grid_data[,"grid"] <- as.factor(grid_data[,"grid"])
grid_data[,"demo"] <- as.factor(grid_data[,"demo"])
# as we know that the data is seriously imbalanced
# so we need to deal with this problem by ROSE
library(ROSE)
grid_data <- ROSE(demo ~ ., data = grid_data, p = 0.5, seed = 1)$data
table(grid_data$demo)
# let split data into train and test
set.seed(1)
train_ind <- sample(x = c(1:dim(grid_data)[1]), size = dim(grid_data)[1] * 0.7)
train_data <- grid_data[train_ind,]
test_data <- grid_data[-train_ind,]
# train_x <- train_data[,1:dim(train_data)[2] - 1]
# train_y <- train_data[dim(train_data)[2]]
library(randomForest)
formula <- "demo ~.-demo"
rf <- randomForest(formula = as.formula(formula), data = train_data,
ntree = 500, importance = TRUE)
plot(rf)
varImpPlot(rf)
importance( x = rf, type = 1)
predict(object = rf, newdata = test_data)
est_demo = predict(object = rf, newdata = test_data)
est_demo
est_demo[,0]
est_demo[,2]
# let's check the performance of opt_tre
accuracy.meas(response = test_data$demo, predicted = est_demo[,2], threshold = 0.5)
# let's check the performance of opt_tre
accuracy.meas(response = test_data$demo, predicted = est_demo, threshold = 0.5)
roc.curve(response = test_data$demo, predicted = est_demo, plotit = T)
# let's check the performance of opt_tre
accuracy.meas(response = test_data$demo, predicted = est_demo, threshold = 0.4)
# let's check the performance of opt_tre
accuracy.meas(response = test_data$demo, predicted = est_demo, threshold = 0.2)
# let's check the performance of opt_tre
accuracy.meas(response = test_data$demo, predicted = est_demo, threshold = 0.6)
# let's check the performance of opt_tre
accuracy.meas(response = test_data$demo, predicted = est_demo, threshold = 0.8)
# let's check the performance of opt_tre
accuracy.meas(response = test_data$demo, predicted = est_demo, threshold = 0.5)
result <- rfcv(trainx = train_data[,-demo], trainy = train_data$demo, cv.fold = 5)
result <- rfcv(trainx = train_data[,-"demo"], trainy = train_data$demo, cv.fold = 5)
result <- rfcv(trainx = subset(train_data, select = -demo), trainy = train_data$demo, cv.fold = 5)
with(result, plot(n.var, error.cv, log="x", type="o", lwd=2))
library(mlr)
#create a task
traintask <- makeClassifTask(data = train_data,target = "demo")
testtask <- makeClassifTask(data = test_data,target = "demo")
traintask
library(mlr)
#create a task
traintask <- makeClassifTask(data = train_data,target = "demo", positive = "1")
testtask <- makeClassifTask(data = test_data,target = "demo", positive = "1")
traintask
#create learner
rf_lrn <- makeLearner("classif.randomForest",predict.type = "response")
rf_lrn$par.vals <- list(ntree = 200L, importance=TRUE)
#set 5 fold cross validation
rdesc <- makeResampleDesc("CV",iters=5L)
# Fit models according to a resampling strategy.
r <- resample(learner = rf_lrn, task = traintask, resampling = rdesc, measures = mmce, show.info = T)
getParamSet(rf_lrn)
# define parameters we want to tune -- you may want to adjust the bounds
ps <- makeParamSet(
makeIntegerLearnerParam(id = "ntree", default = 100L, lower = 80L, upper = 300L),
makeIntegerLearnerParam(id = "nodesize", default = 15L, lower = 10L, upper = 50L),
makeIntegerLearnerParam(id = "mtry", default = 8L, lower = 6, upper = 12L)
)
# random sampling of the configuration space with at most 100 samples
ctrl <- makeTuneControlRandom(maxit = 100L)
tune <- tuneParams(learner = rf_lrn, task = traintask, resampling = rdesc, measures = list(mse), par.set = ps, control = ctrl, show.info = T)
# define parameters we want to tune -- you may want to adjust the bounds
ps <- makeParamSet(
makeIntegerLearnerParam(id = "ntree", default = 100L, lower = 80L, upper = 300L),
makeIntegerLearnerParam(id = "nodesize", default = 15L, lower = 10L, upper = 50L),
makeIntegerLearnerParam(id = "mtry", default = 8L, lower = 6, upper = 12L)
)
# random sampling of the configuration space with at most 100 samples
ctrl <- makeTuneControlRandom(maxit = 100L)
tune <- tuneParams(learner = rf_lrn, task = traintask, resampling = rdesc, measures = list(mmce), par.set = ps, control = ctrl, show.info = T)
# define parameters we want to tune -- you may want to adjust the bounds
ps <- makeParamSet(
makeIntegerLearnerParam(id = "ntree", default = 150L, lower = 100L, upper = 200L),
makeIntegerLearnerParam(id = "nodesize", default = 15L, lower = 10L, upper = 50L),
# makeIntegerLearnerParam(id = "mtry", default = 8L, lower = 6L, upper = 12L)
)
# define parameters we want to tune -- you may want to adjust the bounds
# ps <- makeParamSet(
# makeIntegerLearnerParam(id = "ntree", default = 150L, lower = 100L, upper = 200L),
# makeIntegerLearnerParam(id = "nodesize", default = 15L, lower = 10L, upper = 50L),
# makeIntegerLearnerParam(id = "mtry", default = 8L, lower = 6L, upper = 12L)
# )
ps <- makeParamSet(makeIntegerParam("mtry",lower = 2,upper = 10),makeIntegerParam("nodesize",lower = 10,upper = 50))
# random sampling of the configuration space with at most 100 samples
ctrl <- makeTuneControlRandom(maxit = 100L)
tune <- tuneParams(learner = rf_lrn, task = traintask, resampling = rdesc, measures = list(mmce), par.set = ps, control = ctrl, show.info = T)
# define parameters we want to tune -- you may want to adjust the bounds
ps <- makeParamSet(
makeIntegerLearnerParam(id = "ntree", default = 150L, lower = 100L, upper = 200L),
makeIntegerLearnerParam(id = "nodesize", default = 15L, lower = 10L, upper = 80L),
makeIntegerLearnerParam(id = "mtry", default = 8L, lower = 6L, upper = 12L)
)
# random sampling of the configuration space with at most 100 samples
ctrl <- makeTuneControlRandom(maxit = 100L)
tune <- tuneParams(learner = rf_lrn, task = traintask, resampling = rdesc, measures = list(mmce), par.set = ps, control = ctrl, show.info = T)
opt_rf <- randomForest(formula = as.formula(formula), data = train_data,
ntree = 155, mtry = 8, nodesize = 60, importance = TRUE)
plot(opt_rf)
opt_rf$confusion
est_demo = predict(object = opt_rf, newdata = test_data)
est_demo[,2]
est_demo
# let's check the performance of opt_tre
accuracy.meas(response = test_data$demo, predicted = est_demo, threshold = 0.5)
# let's check the performance of opt_tre
accuracy.meas(response = test_data$demo, predicted = est_demo, threshold = 0.5)
est_demo = predict(object = opt_rf, newdata = test_data)
# let's check the performance of opt_tre
accuracy.meas(response = test_data$demo, predicted = est_demo, threshold = 0.5)
# let's look at roc, auc
roc.curve(response = test_data$demo, predicted = est_demo, plotit = T)
setwd("F:/life in usc/courses/DR/model")
link_data_all <- read.csv(file = 'data/link_data_all.csv', stringsAsFactors = F)
View(link_data_all)
typeof(link_data_all$hour)
link_data_all$hour <- as.factor(link_data_all$hour)
link_data_all$link_id <- as.factor(link_data_all$link_id)
link_data_all$demo <- as.factor(link_data_all$demo)
link_data_all$date <- as.factor(link_data_all$date)
library(ROSE)
link_all_data_balance <- ovun.sample(demo ~ ., data = link_data_all, N = dim(link_data_all)[1]*2, method = 'both')$data
table(link_all_data_balance$demo)
unique(link_all_data_balance$date)
link_all_data_balance$holiday <- as.integer(link_all_data_balance$date %in% c(20161119,20161126,20161210,20161203,20160618))
link_all_data_balance$holiday <- as.factor(link_all_data_balance$holiday)
if(0 <= x <= 5 & 22 <= x<= 23){
link_all_data_balance$hour <- as.integer(link_all_data_balance$hour)
if(0 <= x <= 5 & 22 <= x<= 23){
5 >= 4
print("haha")
if(0 <= x <= 5 | 22 <= x<= 23){
if(0 <= x <= 5 | 22 <= x<= 23){
time_per_fun <- function(x){
if(0<= x <= 5 | 22 <= x<= 23){
x = 22
x = 22
if(0 <= x <= 5 | 22 <= x<= 23){
0 <= x
print x
x = 22
if((0<=x & x <= 5)){
print x
print(x)
x = 22
if((0<=x & x <= 5)){
print(x)
}
if((0<=x & x <= 5) | (22 <= x & x <= 23)){
print(x)
}
'afternoon_commute'
time_per_fun <- function(x){
if((0<=x & x <= 5) | (22 <= x & x <= 23)){
'evening'
}else if((6<= x & x <= 9)){
'morning_commute'
}else if((10 <= x  & x <= 16)){
'working'
}else{
'afternoon_commute'
}
}
link_all_data_balance$time_range <- apply(link_all_data_balance$hour, FUN = time_per_fun)
link_all_data_balance$time_range <- sapply(link_all_data_balance$hour, FUN = time_per_fun)
View(link_all_data_balance)
link_all_data_balance$time_range <- as.factor(link_all_data_balance$time_range)
link_all_data_balance$hour <- as.factor(link_all_data_balance$hour)
link_model_data <- subset(link_all_data_balance,
select = - c(date, link_id, hour, num_vac_taxi, vac_avg_speed))
View(link_model_data)
write.csv(x = link_all_data_balance, file = 'data/link_all_data_balance.csv')
write.csv(x = link_all_data_balance, file = 'data/model_data.csv')
# import link data
link_data <- read.csv(file = "data/model_data.csv", stringsAsFactor = F)
View(link_data)
link_data_all <- read.csv(file = 'data/link_data_all.csv', stringsAsFactors = F)
link_data_all$hour <- as.factor(link_data_all$hour)
link_data_all$link_id <- as.factor(link_data_all$link_id)
link_data_all$demo <- as.factor(link_data_all$demo)
link_data_all$date <- as.factor(link_data_all$date)
library(ROSE)
link_all_data_balance <- ovun.sample(demo ~ ., data = link_data_all, N = dim(link_data_all)[1]*2, method = 'both')$data
table(link_all_data_balance$demo)
unique(link_all_data_balance$date)
link_all_data_balance$holiday <- as.integer(link_all_data_balance$date %in% c(20161119,20161126,20161210,20161203,20160618))
link_all_data_balance$holiday <- as.factor(link_all_data_balance$holiday)
time_per_fun <- function(x){
if((0<=x & x <= 5) | (22 <= x & x <= 23)){
'evening'
}else if((6<= x & x <= 9)){
'morning_commute'
}else if((10 <= x  & x <= 16)){
'working'
}else{
'afternoon_commute'
}
}
link_all_data_balance$time_range <- sapply(link_all_data_balance$hour, FUN = time_per_fun)
link_all_data_balance$hour <- as.integer(link_all_data_balance$hour)
time_per_fun <- function(x){
if((0<=x & x <= 5) | (22 <= x & x <= 23)){
'evening'
}else if((6<= x & x <= 9)){
'morning_commute'
}else if((10 <= x  & x <= 16)){
'working'
}else{
'afternoon_commute'
}
}
link_all_data_balance$time_range <- sapply(link_all_data_balance$hour, FUN = time_per_fun)
link_all_data_balance$time_range <- as.factor(link_all_data_balance$time_range)
link_all_data_balance$hour <- as.factor(link_all_data_balance$hour)
link_model_data <- subset(link_all_data_balance,
select = - c(date, link_id, hour, num_vac_taxi, vac_avg_speed))
write.csv(x = link_model_data, file = 'data/model_data.csv')
# import link data
link_data <- read.csv(file = "data/model_data.csv", stringsAsFactor = F)
View(link_data)
# import link data
link_data <- read.csv(file = "data/model_data.csv", stringsAsFactor = F, row.names = F)
# import link data
link_data <- read.csv(file = "data/model_data.csv", stringsAsFactor = F, row.names = FALSE)
# import link data
link_data <- read.csv(file = "data/model_data.csv", stringsAsFactor = F)[:-1]
# import link data
link_data <- read.csv(file = "data/model_data.csv", stringsAsFactor = F)[-1]
View(link_data)
#
# head(link_data)
str(link_data)
# summary(link_data)
link_data$demo <- as.factor(link_data$demo)
link_data$holiday <- as.factor(link_data$holiday)
link_data$time_range <- as.factor(link_data$holiday)
summary(link_data)
View(link_model_data)
# import link data
link_data <- read.csv(file = "data/model_data.csv", stringsAsFactor = F)[-1]
View(link_model_data)
#
# head(link_data)
str(link_data)
# summary(link_data)
link_data$demo <- as.factor(link_data$demo)
link_data$holiday <- as.factor(link_data$holiday)
link_data$time_range <- as.factor(link_data$time_range)
summary(link_data)
# missing value?
sapply(link_data, function(x){sum(is.na(x))})
table(link_data[,"demo"])
#we know the data set is seriously imbalanced
# let split data into train and test
set.seed(1)
train_ind <- sample(x = c(1:dim(link_data)[1]), size = dim(link_data)[1] * 0.7)
train_data <- link_data[train_ind,]
test_data <- link_data[-train_ind,]
dim(train_data)
# let's first use rpart to train a decision tr
library(rpart)
tree <- rpart(formula = "demo ~. -demo", data = train_data,
method = "class",
control = rpart.control(minsplit = 50, cp = 0.05))
# look at what the tree is like
tree
cand_data <- link_data
# if we drop holiday
link_data <- subset(cand_data, selection = -holidday)
# let split data into train and test
set.seed(1)
train_ind <- sample(x = c(1:dim(link_data)[1]), size = dim(link_data)[1] * 0.7)
train_data <- link_data[train_ind,]
test_data <- link_data[-train_ind,]
View(train_data)
# if we drop holiday
link_data <- subset(cand_data, selection = -holiday)
View(cand_data)
# if we drop holiday
link_data <- subset(cand_data, selection = -holiday)
cand_data <- link_data
# if we drop holiday
link_data <- subset(cand_data, selection = -holiday)
# if we drop holiday
link_data <- subset(link_data, selection = -holiday)
# if we drop holiday
link_data <- subset(cand_data, selection = -holiday)
# if we drop holiday
link_data <- subset(cand_data, selection = -holiday)
# if we drop holiday
link_data <- subset(cand_data, selection = -holiday)
# if we drop holiday
link_data <- subset(cand_data,select = - holiday)
# let split data into train and test
set.seed(1)
train_ind <- sample(x = c(1:dim(link_data)[1]), size = dim(link_data)[1] * 0.7)
train_data <- link_data[train_ind,]
test_data <- link_data[-train_ind,]
dim(train_data)
# let's first use rpart to train a decision tr
library(rpart)
tree <- rpart(formula = "demo ~. -demo", data = train_data,
method = "class",
control = rpart.control(minsplit = 50, cp = 0.05))
# look at what the tree is like
tree
# if we drop holiday
link_data <- subset(cand_data,select = - time_range)
# let split data into train and test
set.seed(1)
train_ind <- sample(x = c(1:dim(link_data)[1]), size = dim(link_data)[1] * 0.7)
train_data <- link_data[train_ind,]
test_data <- link_data[-train_ind,]
dim(train_data)
# let's first use rpart to train a decision tr
library(rpart)
tree <- rpart(formula = "demo ~. -demo", data = train_data,
method = "class",
control = rpart.control(minsplit = 50, cp = 0.05))
# look at what the tree is like
tree
# if we drop holiday
link_data <- subset(cand_data,select = - c(time_range,holiday))
# let split data into train and test
set.seed(1)
train_ind <- sample(x = c(1:dim(link_data)[1]), size = dim(link_data)[1] * 0.7)
train_data <- link_data[train_ind,]
test_data <- link_data[-train_ind,]
dim(train_data)
# let's first use rpart to train a decision tr
library(rpart)
tree <- rpart(formula = "demo ~. -demo", data = train_data,
method = "class",
control = rpart.control(minsplit = 50, cp = 0.05))
# look at what the tree is like
tree
# xerror: error in cross validation
# xstd: standard deviation of error in cross vaidation
printcp(tree)
# let's first use rpart to train a decision tr
library(rpart)
tree <- rpart(formula = "demo ~. -demo", data = train_data,
method = "class",
control = rpart.control(minsplit = 20, cp = 0.05))
# look at what the tree is like
tree
# xerror: error in cross validation
# xstd: standard deviation of error in cross vaidation
printcp(tree)
# let's have a look at complexity parameter against xerror
plotcp(tree)
# optimal cp
cptable <- as.data.frame(tree$cptable)
opt_cp <- cptable[with(cptable, min(which((xerror - xstd) < min(xerror)))), "CP"]
opt_tree <- prune(tree = tree, cp = opt_cp)
# use it on test data
est_prob <- predict(object = opt_tree, newdata = test_data)
#
accuracy.meas(response = test_data$demo, predicted = est_prob[,2], threshold = 0.5)
# let's look at roc, auc
roc.curve(response = test_data$demo, predicted = est_prob[,2], plotit = T)
#
accuracy.meas(response = test_data$demo, predicted = est_prob[,2], threshold = 0.6)
#
accuracy.meas(response = test_data$demo, predicted = est_prob[,2], threshold = 0.4)
link_model_data <- subset(link_all_data_balance,
select = - c(date, link_id, hour, num_taxi, avg_speed))
colnames(link_model_data)
write.csv(x = link_model_data, file = 'data/model_data.csv')
# import link data
link_data <- read.csv(file = "data/model_data.csv", stringsAsFactor = F)[-1]
#
# head(link_data)
str(link_data)
# summary(link_data)
link_data$demo <- as.factor(link_data$demo)
link_data$holiday <- as.factor(link_data$holiday)
link_data$time_range <- as.factor(link_data$time_range)
summary(link_data)
# missing value?
sapply(link_data, function(x){sum(is.na(x))})
cand_data <- link_data
# if we drop holiday
link_data <- subset(cand_data,select = - c(time_range,holiday))
# let split data into train and test
set.seed(1)
train_ind <- sample(x = c(1:dim(link_data)[1]), size = dim(link_data)[1] * 0.7)
train_data <- link_data[train_ind,]
test_data <- link_data[-train_ind,]
dim(train_data)
# let's first use rpart to train a decision tr
library(rpart)
tree <- rpart(formula = "demo ~. -demo", data = train_data,
method = "class",
control = rpart.control(minsplit = 20, cp = 0.05))
# look at what the tree is like
tree
# let's first use rpart to train a decision tr
library(rpart)
tree <- rpart(formula = "demo ~. -demo", data = train_data,
method = "class",
control = rpart.control(minsplit = 20, cp = 0))
# look at what the tree is like
tree
# xerror: error in cross validation
# xstd: standard deviation of error in cross vaidation
printcp(tree)
# let's have a look at complexity parameter against xerror
plotcp(tree)
# optimal cp
cptable <- as.data.frame(tree$cptable)
opt_cp <- cptable[with(cptable, min(which((xerror - xstd) < min(xerror)))), "CP"]
opt_tree <- prune(tree = tree, cp = opt_cp)
# use it on test data
est_prob <- predict(object = opt_tree, newdata = test_data)
#
accuracy.meas(response = test_data$demo, predicted = est_prob[,2], threshold = 0.5)
# let's look at roc, auc
roc.curve(response = test_data$demo, predicted = est_prob[,2], plotit = T)
# let's first use rpart to train a decision tr
library(rpart)
tree <- rpart(formula = "demo ~. -demo", data = train_data,
method = "class",
control = rpart.control(minsplit = 20, cp = 0.01))
# look at what the tree is like
tree
# xerror: error in cross validation
# xstd: standard deviation of error in cross vaidation
printcp(tree)
# let's have a look at complexity parameter against xerror
plotcp(tree)
# optimal cp
cptable <- as.data.frame(tree$cptable)
opt_cp <- cptable[with(cptable, min(which((xerror - xstd) < min(xerror)))), "CP"]
opt_tree <- prune(tree = tree, cp = opt_cp)
# use it on test data
est_prob <- predict(object = opt_tree, newdata = test_data)
#
accuracy.meas(response = test_data$demo, predicted = est_prob[,2], threshold = 0.5)
# let's look at roc, auc
roc.curve(response = test_data$demo, predicted = est_prob[,2], plotit = T)
# let's first use rpart to train a decision tr
library(rpart)
tree <- rpart(formula = "demo ~. -demo", data = train_data,
method = "class",
control = rpart.control(minsplit = 20, cp = 0))
# look at what the tree is like
tree
# xerror: error in cross validation
# xstd: standard deviation of error in cross vaidation
printcp(tree)
# let's have a look at complexity parameter against xerror
plotcp(tree)
# optimal cp
cptable <- as.data.frame(tree$cptable)
opt_cp <- cptable[with(cptable, min(which((xerror - xstd) < min(xerror)))), "CP"]
opt_tree <- prune(tree = tree, cp = opt_cp)
# use it on test data
est_prob <- predict(object = opt_tree, newdata = test_data)
#
accuracy.meas(response = test_data$demo, predicted = est_prob[,2], threshold = 0.5)
# let's look at roc, auc
roc.curve(response = test_data$demo, predicted = est_prob[,2], plotit = T)
